from src.evaluation.evaluate_rag import RAGEvaluator, EvaluationExample
from src.evaluation.metrics import MockMetric

def test_run():
    # 1. Creamos datos de prueba (Simulando lo que saldrÃ­a de tu RAG)
    ejemplos = [
        EvaluationExample(
            query="Â¿CuÃ¡l es la carga para fuerza mÃ¡xima?",
            answer="La NSCA sugiere cargas superiores al 85% del 1RM.",
            contexts=["CapÃ­tulo 15: Entrenamiento de resistencia, pÃ¡g 395."],
            trace_id="test_001"
        ),
        EvaluationExample(
            query="Â¿QuÃ© es el OIB?",
            answer="Es un tÃ©rmino de prueba.",
            contexts=["Documento tÃ©cnico 1"],
            trace_id="test_002"
        )
    ]

    # 2. Inicializamos el evaluador con nuestra mÃ©trica de prueba
    metrics = [MockMetric()]
    evaluator = RAGEvaluator(metrics=metrics)

    # 3. Ejecutamos la evaluaciÃ³n
    print("ðŸš€ Iniciando prueba del Evaluador...")
    report = evaluator.evaluate(ejemplos)
    
    print("âœ… Prueba completada con Ã©xito.")

if __name__ == "__main__":
    test_run()